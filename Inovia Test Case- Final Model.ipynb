{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "#from scipy.stats import skew\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Euclidean distance\n",
    "def measureDistance(data, model):\n",
    "    distance = pd.Series()\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]        \n",
    "        distance.set_value(i, np.linalg.norm(Xa-Xb))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(logFileName):\n",
    "    colNames = ['ip','dp1','dp2','date','time','numeric1','rest','rc','numeric2','dp3','client','in','out','us']\n",
    "    logData = pd.read_csv('access_logs_201612.txt', delim_whitespace=True, header=None, names=colNames, parse_dates=[3,4])\n",
    "    #drop unused columns\n",
    "    logData = logData.drop(['dp1','dp2','dp3'], axis =1)\n",
    "    return logData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanInOutUs(logData):\n",
    "    logData['in']= logData['in'].apply(lambda x : int(x.split(\":\")[1]))\n",
    "    logData['out']= logData['out'].apply(lambda x : int(x.split(\":\")[1]))\n",
    "    logData['us']= logData['us'].apply(lambda x : int(x.split(\":\")[1]))\n",
    "    return logData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractResourceOp(restserv):\n",
    "    arr = restserv.split(\" \")\n",
    "    op = arr[0]\n",
    "    url = arr[1]\n",
    "    protocol=arr[2]\n",
    "    resource=url.split(\"?\")[0]\n",
    "    return [op, resource,protocol]\n",
    "\n",
    "\n",
    "def extractOp(restserv):\n",
    "    arr = restserv.split(\" \")\n",
    "    op = arr[0]\n",
    "    return [op]\n",
    "\n",
    "def parseRestURL(logData):\n",
    "    logData[['operation']] = logData.apply(lambda row:pd.Series(extractOp(row['rest'])),axis=1)\n",
    "    return logData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDateFeatures(row):   \n",
    "    day = row.dayofweek\n",
    "    return day\n",
    "\n",
    "def parseDate(logData):\n",
    "    logData['day'] = logData['date'].apply(lambda x : extractDateFeatures(x))\n",
    "    return (logData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTimeFeatures(row):\n",
    "    hour = row.hour\n",
    "    minute = row.minute\n",
    "    return [hour,minute]\n",
    "\n",
    "def parseTime(logData):\n",
    "    logData[['hour', 'minute']] = logData.apply(lambda row:pd.Series(extractTimeFeatures(row['time'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracFirstIpPart(x):\n",
    "    return x.split(\".\")[0]\n",
    "\n",
    "def parseIP(logData):\n",
    "    logData['first_ip_part'] = logData['ip'].apply (lambda x : extracFirstIpPart(x))\n",
    "    return logData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapIpToColumn(part):   \n",
    "    if (part.startswith(\"10\")):\n",
    "        return [0, 0, 0]\n",
    "    if (part.startswith(\"100\")):\n",
    "        return [0, 0, 1]\n",
    "    if (part.startswith(\"134\")):\n",
    "        return [0, 1, 0]    \n",
    "    if (part.startswith(\"137\")):\n",
    "        return [0, 1,  1]\n",
    "    if (part.startswith(\"147\")):\n",
    "        return [1 ,0, 0]\n",
    "    if (part.startswith(\"150\") | part.startswith(\"153\")):\n",
    "        return [1, 0, 1]\n",
    "    if (part.startswith(\"localhost\") ):\n",
    "        return [1, 1, 0]\n",
    "    return [1,1,1] \n",
    "\n",
    "def transformIP_to_DummyVar(logData):\n",
    "    logData[['ip_3', 'ip_2', 'ip_1']] = logData.apply(lambda row:pd.Series(mapIpToColumn(row['first_ip_part'])),\n",
    "                                                           axis=1)\n",
    "    return logData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapOperationToColumn(part):   \n",
    "    if (part.startswith(\"DELETE\")):\n",
    "        return [0, 0, 0]\n",
    "    if (part.startswith(\"GET\")):\n",
    "        return [0, 0, 1]\n",
    "    if (part.startswith(\"OPTIONS\")):\n",
    "        return [0, 1, 0]    \n",
    "    if (part.startswith(\"POST\")):\n",
    "        return [0, 1,  1]\n",
    "    if (part.startswith(\"PUT\")):\n",
    "        return [1 ,0, 0]   \n",
    "    return [1, 0, 1]   \n",
    "\n",
    "def transformOperation_to_DummyVar(logData):\n",
    "    logData[['op_3', 'op_2', 'op_1']] = logData.apply(lambda row:pd.Series(mapOperationToColumn(row['operation'])),\n",
    "                                                           axis=1)\n",
    "    return logData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData_PCA(logData):\n",
    "    # We take only numeric feature and standardize them\n",
    "    pcaData = logData.select_dtypes(include=[np.int64, np.float64])\n",
    "    pcaData = pcaData.drop ([\"numeric1\"], axis=1)\n",
    "\n",
    "    min_max_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(pcaData)\n",
    "    pcaData = pd.DataFrame(np_scaled)\n",
    "\n",
    "    # I reduce to 2 importants features (for the sake of visualization)\n",
    "    pca = PCA(n_components=2)\n",
    "    pcaData = pca.fit_transform(pcaData)\n",
    "    # And standardizing these 2 new features\n",
    "    min_max_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(pcaData)\n",
    "    pcaData = pd.DataFrame(np_scaled)\n",
    "    return pcaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As this process take long time in my laptop, I chunked into pieces and print the results in each chunk\n",
    "def tagRowsWithAnomalies(logData, pcaData, outliers_fraction, kmeans):\n",
    "    nrows = int(logData[[0,1]].shape[0])\n",
    "    piece = int( nrows / 100)\n",
    "    outliers_fraction = 0.01\n",
    "    \n",
    "    for x in range(0, 101):\n",
    "        lower = x * piece\n",
    "        upper =min(((x+1) * piece), nrows)       \n",
    "\n",
    "        data_range= pcaData[lower:upper]\n",
    "        data_range =  data_range.reset_index()\n",
    "        data_range =  data_range.drop(['index'],1)\n",
    "        distance = measureDistance(data_range, kmeans)\n",
    "       \n",
    "        number_of_outliers = int(outliers_fraction*len(distance))\n",
    "        threshold = distance.nlargest(number_of_outliers).min()\n",
    "        # (0:normal, 1:anomaly)  \n",
    "        s = (distance >= threshold).astype(int)    \n",
    "        logData.loc[lower:upper,\"anomaly\"] = s\n",
    "        if (x % 10 == 0):\n",
    "            print (\"chunk \", x , \" is done\")\n",
    "\n",
    "    print (\"Finding anomalies is finished\")\n",
    "    return (logData)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main functiuon to find anomalies\n",
    "def findAnomalies(logfileName, outliers_fraction, numCluster):\n",
    "    logData= loadFile(logfileName)\n",
    "    logData = cleanInOutUs(logData)\n",
    "    logData = parseRestURL(logData)\n",
    "    logData = parseDate(logData)\n",
    "\n",
    "    #Transform categorical features into binary dummy variable\n",
    "    logData = parseIP(logData)\n",
    "    logData = transformIP_to_DummyVar(logData)\n",
    "    logData = transformOperation_to_DummyVar(logData)\n",
    "\n",
    "    pcaData = normalizeData_PCA(logData)\n",
    "    kmeans = KMeans(n_clusters=numCluster).fit(data)\n",
    "\n",
    "    logData['cluster'] = kmeans.predict(pcaData)\n",
    "    logData['principal_feature1'] = pcaData[0]\n",
    "    logData['principal_feature2'] = pcaData[1]\n",
    "    logData = tagRowsWithAnomalies(logData, pcaData, outliers_fraction, kmeans)\n",
    "    return logData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfileName='access_logs_201612.txt'\n",
    "numCluster=4\n",
    "outliers_fraction=0.01\n",
    "#  outliers_fraction: An estimation of anomly population of the dataset \n",
    "findAnomalies(logfileName, outliers_fraction, numCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
